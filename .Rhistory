# TCSS 588: you don't need to evaluate your method using BAC and AUC
# just compute the number of errors is sufficient.
# for TCSS 588 hw3, change functions "evalMethod", "bac", "auc"
evalMethod<-function(pred,bagging,th)
{
round=max(pred$round)
fold=max(pred$fold)
#columns number respect to boosting
col=4+bagging
#average
pred$avg<-rowMeans(pred[,5:col])
#calculate majority
pred$countth<-rowSums(pred[,5:col]>th)
pred$majority<-ifelse(pred$countth>(bagging/2),1,0)
#Variables for temprary result
BAC <- NULL
AUC<-NULL
AUC.avg<-NULL
BAC.avg<-NULL
BAC.majority<-NULL
for(j in 1:round)
{
#for averaging the BAC
aBAC <- NULL
aAUC <- NULL
aBAC.avg<-NULL
aAUC.avg<-NULL
aBAC.majority<-NULL
for(i in 1:fold)
{
cat("\n round ", j ," fold: ", i )
evaluation.data<- subset(pred, pred$round==j & pred$fold==i)
#scoring on testing fold
#AUC without bagging
foldAUC <- auc(evaluation.data$trainingClass,evaluation.data$p)
aAUC <- c(aAUC,foldAUC)
#BAC without bagging
foldBAC <- bac(evaluation.data$trainingClass,evaluation.data$p,th)
aBAC <- c(aBAC,foldBAC)
#BAC with bagging avg
foldBAC.avg <- bac(evaluation.data$trainingClass,evaluation.data$avg,th)
aBAC.avg <- c(aBAC.avg,foldBAC.avg)
#AUC with bagging-avg
foldAUC.avg <- auc(evaluation.data$trainingClass,evaluation.data$avg)
aAUC.avg <- c(aAUC.avg,foldAUC.avg)
#BAC with bagging- majority
foldBAC.majority <- bac(evaluation.data$trainingClass,evaluation.data$majority,th)
aBAC.majority <- c(aBAC.majority,foldBAC.majority)
#show statistics in output
cat("\nfold " , i , "round " , j ,"AUC: ", foldAUC)
cat("\nfold " , i , "round " , j ,"bac: ", foldBAC)
cat("\nfold " , i , "round " , j ,"bac of avg: ", foldBAC.avg)
cat("\nfold " , i , "round " , j ,"AUC of avg: ", foldAUC.avg)
cat("\nfold " , i , "round " , j ,"bac of majority: ", foldBAC.majority,"\n\n")
}
#Scoring on whole round
AUC<-c(AUC,mean(aAUC))
AUC.avg<-c(AUC.avg,mean(aAUC.avg))
BAC <- c(BAC,mean(aBAC))
BAC.avg <- c(BAC.avg,mean(aBAC.avg))
BAC.majority <- c( BAC.majority,mean(aBAC.majority))
}
result <- data.frame(AUC,AUC.avg,BAC,BAC.avg,BAC.majority)
names(result) <- c("AUC.NOBagging","AUC.Bagging-avg","BAC.NoBagging","BAC.Bagging-avg","BAC.Bagging-majority")
return (result)
}
bac <- function(targets, predictions ,th)
{
#Calculating confusion matrix based on threshold
prediction<- data.frame(targets,predictions)
tp <- nrow(prediction[which(prediction$target == 1 & prediction$predictions>=th),])
fn <- nrow(prediction[which(prediction$target == 1 & prediction$predictions <th ),])
fp <- nrow(prediction[which(prediction$target == 0 & prediction$predictions >= th),])
tn <- nrow(prediction[which(prediction$target == 0 & prediction$predictions <th),])
#Calculating Bac
class1<-ifelse((tp+fn==0),0,tp/(tp+fn))
class0<-ifelse((tn+fp==0),0,tn/(tn+fp))
BAC<-(1/2) *(class1 + class0 )
#Print statistics
cat("\nConfusion Matrix"," tp:", tp, " Fp:", fp," tn:", tn
," fn:", fn," sum:",(tp+fp+fn+tn))
cat("\nfor positive case (majority) ",  tp/(tp+fn))
cat("\nfor negative case (minority) ", tn/(tn+fp))
return(BAC)
}
auc<-function(targets, predictions)
{
AUC.pROC <- roc(targets,predictions)$auc
cat( "  Auc pROC: ",AUC.pROC )
return(AUC.pROC)
}
#==================================================Model=================
#      it gets train and test data as input to train the model and return the prediction result
#      Model output is supposed to be probabilities not the class label
nb<-function(trainingData,testingData)
{
NB.model<-naiveBayes(trainingClass~.,trainingData)
prediction <- round(predict( NB.model,testingData,type="raw")[,2],5)
return(prediction)
}
##================================================formatting the data
dataFormat<-function(datafileDir)
{
data.raw <- read.csv(datafileDir)
trainingData <- data.raw[,-(1:(which(colnames(data.raw)=="ACTB")-1))]
trainingClass <- rep(0,nrow(trainingData))
for(i in 1:length(trainingClass))
{
if(data.raw$resp.simple[i]=="CR")
trainingClass[i] =1
else
trainingClass[i]=0
}
data<-data.frame(trainingClass,trainingData)
data$trainingClass<-as.factor(data$trainingClass)
return(data)
}
data <- cars
set.seed(42)
result <- kfoldCV(data,3,nb,5,5,40)
mydata <- cars
mydata <- cars[sample(nrow),]
mydata <- cars[sample(nrow(mydata)),]
folds <- cut(seq(1,nrow(mydata)),breaks = 10,labels = FALSE)
for(i in 1:10){}
for(i in 1:10){
testIndexes <- which(folds == i,arr.ind = TRUE)
testdata <- mydata[testIndexes,]
traindata <- mydata[-testIndexes,]
traindata <- mydata[-testIndexes,]
random <- randomForest(speed~., data = cars. importance = TRUE, proximity = TRUE)
.libPaths()
.libPaths(/Users/amruthaa)
.libPaths(/Users/amruthaa)
.libPaths("Users/amruthaa")
setwd("Users/amruthaa")
getwd
getwd()
install.packages("robustbase", lib = "Users/amruthaa")
install.packages("acepack", lib = "Users/amruthaa")
install.packages("ada",lib = "Users/amruthaa")
install.packages("amsterRdam",lib = "Users/amruthaa")
install.packages("AnnotationDbi", lib = "Users/amruthaa")
install.packages("acepack", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("ada", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("amsterRdam", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("AnnotationDbi", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("assertthat", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("backports", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("base64enc", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("BH", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("bindr", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("bindrcpp", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("Biobase", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("BioGenerics", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("bitops", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("caretEnsemble", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("bioconductor", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
source("https://bioconductor.org/biocLite.R")
biocLite()
source("https://bioconductor.org/biocLite.R")
biocLite("bridge")
source("https://bioconductor.org/biocLite.R")
biocLite("breastCancerNKI")
source("https://bioconductor.org/biocLite.R")
biocLite("impute")
install.packages("class", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
install.packages("e1071", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
setwd("~/data/ATPG")
library(randomForest)
mydata <- read.table("ATPG.csv", header = TRUE, sep = "\n")
mydata
header <- c('BDD_sub_cir_only','Interferance', 'QMDD_Property', 'Control_Target_only', 'Control_Line_Reduction(TBS)','Algorithm')
mydata$Algorithm
mydata <- read.csv(file="ATPG.csv", sep=" ", colClasses=c("NULL", NA, NA))
setwd("~/Downloads/csv")
mydata <- read.csv(file="out_TBS.csv",header = TRUE, sep=" ")
mydata <- read.csv(file="out_QMDD.csv",header = TRUE, sep=" ")
mydata$BDD_sub_cir_only.Interferance.QMDD_Property.Control_Target_only.Control_Line_Reduction.TBS.
setwd("~/Desktop")
library(randomForest)
mydata <- read.csv(file="Workbook3.csv",header = TRUE, sep=" ")
setwd("~/")
my.data <- read.table("ATPG.txt", header = TRUE, sep = '\t')
setwd("~/data/ATPG")
my.data <- read.table("ATPG.csv", header = TRUE, sep = '\t')
my.data$BDD_sub_cir_only.Interferance.QMDD_Property.Control_Target_only.Control_Line_Reduction.TBS..Algorithm
my.data <- read.table("ATPG.csv", header = TRUE, sep = " ")
my.data[1,]
my.data[1,]
my.data[,4]
my.data[[5]]
library(randomForest)
my.data <- read.csv("ATPG1.csv", header = TRUE, sep = "")
setwd("~/data/ATPG")
my.data <- read.csv("ATPG1.csv", header = TRUE, sep = "")
setwd("~/")
my.data <- read.csv("ATPG1.csv", header = TRUE, sep = "")
my.data <- read.csv("ATPG1.csv", header = TRUE, sep = ".")
my.data <- read.csv("ATPG1.csv", header = TRUE, sep = ".")
my.data <- read.csv("ATPG1.csv", header = TRUE, sep = '.')
my.data <- read.table("ATPG1.csv", header = TRUE, sep = '.')
library(randomForest)
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm
my.data$Algorithm <- as.factor(my.data$Algorithm)
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)/2))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
test <- df[my_sample, ]
train <- df[my_sample_comp, ]
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
library(ggplot2)
summary(r)
class(r)
print(r)
predict(r, my.data[ind == 2,], predict.all=TRUE)
predict(r, my.data$Algorithm, predict.all=TRUE)
predict(r, data = test)
summary(r)
print(r)
source('~/ATPG_random.R')
source('~/Desktop/ATPG_random.R')
getwd
getwd()
library(randomForest)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)/2))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
#splitting to train and test data
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
#random forest algorithm
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100,)
summary(r)
#error rate is 44.22%
print(r)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
library(caret)
flds <- createFolds(y, k = 10, list = TRUE, returnTrain = FALSE)
rf.cv <- rf.crossValidation(r, test, p=0.10, n=10, ntree=501)
library(randomForest)
summary(r)
rf.cv <- rf.crossValidation(r, test, p=0.10, n=10, ntree=501)
require(randomForest)
rf.cv <- rf.crossValidation(r, test, p=0.10, n=10, ntree=501)
library(randomForest)
library(caret)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
#splitting to train and test data
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
#random forest algorithm
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100,)
summary(r)
#rf.cv <- rf.crossValidation(r, test, p=0.10, n=10, ntree=501)
#error rate is 44.22%
print(r)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
library(randomForest)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
#splitting to train and test data
test <- my.data[-my_sample, ]
train <- my.data[my_sample, ]
#random forest algorithm
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100,)
summary(r)
#error rate is 44.22%
print(r)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
library(randomForest)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
#splitting to train and test data
test <- my.data[-my_sample, ]
train <- my.data[my_sample, ]
#random forest algorithm
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
summary(r)
#error rate is 44.22%
print(r)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
library(randomForest)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
#splitting to train and test data
test <- my.data[my_sample, ]
train <- my.data[-my_sample, ]
#random forest algorithm
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
summary(r)
#error rate is 44.22%
print(r)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
install.packages("rfUtilities", lib="/Library/Frameworks/R.framework/Versions/3.4/Resources/library")
library(rfUtilities)
rf.crossValidation(r, test$Algorithm, p=0.1, n= 10, seed = NULL, normalize = FALSE)
rf.crossValidation(r, test, p=0.1, n= 10, seed = NULL, normalize = FALSE)
rf.crossValidation(r, p=0.1, n= 10, seed = NULL, normalize = FALSE)
rf.crossValidation(r, my.data$Algorithm, p=0.1, n= 10, seed = NULL, normalize = FALSE)
rf.crossValidation(r, Algorithm, p=0.1, n= 10, seed = NULL, normalize = FALSE)
rf.crossValidation(r, my.data$Algorithm, p=0.1, n= 10, seed = NULL, normalize = FALSE)
r <- randomForest(train$Algorithm ~ ., data=train, importance=TRUE, do.trace=100,)
r <- randomForest(my.data$Algorithm ~ ., data=train, importance=TRUE, do.trace=100,)
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100,)
#random forest algorithm
r <- randomForest(my.data$Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
r <- randomForest(train$Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
rf.crossValidation(r, train$Algorithm, p=0.1, n= 10, seed = NULL, normalize = FALSE)
rf.crossValidation(r, xdata = train$Algorithm, p=0.1, n= 10, seed = NULL, normalize = FALSE)
library(plyr)
library(randomForest)
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
k = 5 #Folds
my.data <- sample(1:k, nrow(data), replace = TRUE)
list <- 1:k
library(plyr)
library(randomForest)
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
k = 5 #Folds
my.data <- sample(1:k, nrow(my.data), replace = TRUE)
list <- 1:k
prediction <- data.frame()
testsetCopy <- data.frame()
progress.bar <- create_progress_bar("text")
progress.bar$init(k)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
my.data$id <- sample(1:k, nrow(my.data), replace = TRUE)
rf.crossValidation(r, train, p = 0.1, n = 10)
r <- randomForest(train$Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
summary(r)
rcv <- rf.crossValidation(r, train, p = 0.1, n = 10)
print(rcv)
#plot
par(mfrow=c(1,2))
plot(rcv, type = "cv", main = "CV producers accuracy")
plot(rcv, type = "model", main = "Model producers accuracy")
#plot cv vs model oob
# Plot cross validation verses model oob
par(mfrow=c(1,2))
plot(rcv, type = "cv", stat = "oob", main = "CV oob error")
plot(rcv, type = "model", stat = "oob", main = "Model oob error")
plot(rcv)
plot(rcv, stat = "mse")
plot(rcv, stat = "mse")
plot(rcv, stat = "var.exp")
plot(rcv, stat = "mae")
plot(r)
print(r)
summary(rcv)
summary(r)
#random forest algorithm
r <- randomForest(train$Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
library(randomForest)
library(caret)
library(rfUtilities)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
#splitting to train and test data
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
#random forest algorithm
r <- randomForest(train$Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
summary(r)
rcv <- rf.crossValidation(r, train, p = 0.1, n = 10)
#error rate is 39.34%
print(r)
print(rcv)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
#plot
plot(rcv)
plot(r)
library(randomForest)
library(caret)
library(rfUtilities)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
#splitting to train and test data
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
#random forest algorithm
r <- randomForest(train$Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
summary(r)
rcv <- rf.crossValidation(r, train, p = 0.1, n = 10)
#error rate is 39.34%
print(r)
print(rcv)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
#plot
plot(r)
plot(rcv)
plot(r)
setwd("~/Desktop/amru winter quarter/Other works/ATPG")
#error rate is 39.34%
print(r)
library(randomForest)
library(caret)
library(rfUtilities)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
#splitting to train and test data
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
#random forest algorithm
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
summary(r)
#rcv <- rf.crossValidation(r, train, p = 0.1, n = 10)
#10 fold cross validation
folds <- cut(seq(1,nrow(my.data)),breaks=10,labels=FALSE)
for(k in 1:10)
{
test_i <- which(folds == k)
train_data <- my.data[-test_i, ]
test_data <- my.data[test_i, ]
rcv <- randomForest(Algorithm~., data = train_data, importance = TRUE, do.trace = 100)
#summary(rcv)
}
#error rate is 39.34%
print(r)
print(rcv)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
#plot
plot(r)
plot(rcv)
library(randomForest)
library(caret)
library(rfUtilities)
#loading data
my.data <- read.table("ATPG1.csv", header = TRUE, sep = ',')
my.data$Algorithm <- as.factor(my.data$Algorithm)
#preparing the data
set.seed(31)
my_sample <- sort(sample(x = 1:nrow(my.data), replace = FALSE, size = nrow(my.data)*0.90))
my_sample_comp <- setdiff(1:nrow(my.data), my_sample)
#splitting to train and test data
test <- my.data[my_sample, ]
train <- my.data[my_sample_comp, ]
#random forest algorithm
r <- randomForest(Algorithm ~ ., data=train, importance=TRUE, do.trace=100)
summary(r)
#rcv <- rf.crossValidation(r, train, p = 0.1, n = 10)
#10 fold cross validation
folds <- cut(seq(1,nrow(my.data)),breaks=10,labels=FALSE)
for(k in 1:10)
{
test_i <- which(folds == k)
train_data <- my.data[-test_i, ]
test_data <- my.data[test_i, ]
rcv <- randomForest(Algorithm~., data = train_data, importance = TRUE, do.trace = 100)
print(rcv)
}
#error rate is 39.34%
print(r)
print(rcv)
#prediction result for test data. here you can use the data to be predicted as test data
predict(r, data = test)
#plot
plot(r)
plot(rcv)
